import os
import pandas as pd

# ========= USER INPUTS =========
# Source file containing case extension data
SOURCE_FILE = r"D:\UATMigration\Source\CaseExtension_Source.csv"

# Lookup files
USER_LOOKUP_FILE = r"D:\UATMigration\Lookup Files\User_Lkp.csv"
ACCOUNT_LOOKUP_FILE = r"D:\UATMigration\Lookup Files\Account_Lkp.xlsx"
CONTACT_LOOKUP_FILE = r"D:\UATMigration\Lookup Files\merged_contactId_Lkp.csv"
CASE_LOOKUP_FILE = r"D:\UATMigration\Lookup Files\Case_Lkp.csv"

# Output directory
OUTPUT_DIR = r"D:\UATMigration\Output"

# ========= CONSTANTS =========
DEFAULT_OWNER_ID = "005Wr00000ECxAjIAL"
DEFAULT_CREATEDBY_LASTMODIFIED_ID = "005A0000000rXeVIAU"
BUSINESS_HOURS_ID = "01mWr0000012RrmIAE"
CHUNK_SIZE = 50_000

# Columns to exclude from output
EXCLUDE_COLUMNS = ["RecordTypeId", "Account.RecordType.Name", "Account.Recordtype.Name"]

# ========= END OF USER INPUTS =========


def load_simple_lookup(path, key_col="Legacy_SF_Record_ID__c", value_col="Id"):
    """Load a simple key-value lookup file (Account, Contact, Case)"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Lookup file not found: {path}")
    
    if path.lower().endswith((".xls", ".xlsx")):
        df = pd.read_excel(path, dtype=str)
    else:
        df = pd.read_csv(path, dtype=str)
    
    df = df.fillna("")
    
    missing = {key_col, value_col} - set(df.columns)
    if missing:
        raise ValueError(f"Missing column(s) in {path}: {', '.join(missing)}")
    
    # Strip whitespace and build lowercase key dictionary
    for col in df.columns:
        df[col] = df[col].astype(str).str.strip()
    
    return {
        str(k).strip().lower(): str(v).strip()
        for k, v in zip(df[key_col], df[value_col])
        if str(k).strip()
    }


def load_user_lookup(path):
    """Load user lookup file and return mapping function"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"User lookup file not found: {path}")
    
    df = pd.read_csv(path, dtype=str).fillna("")
    
    # Strip whitespace from all columns
    for col in df.columns:
        df[col] = df[col].astype(str).str.strip()
    
    # Build mapping dictionaries
    dict_digital_to_global = {
        str(k).lower(): str(v)
        for k, v in zip(df["digital_prod_Id"], df["digital_Global_ID__c"])
        if str(k).strip()
    }
    
    dict_global_to_merge = {
        str(k).lower(): str(v)
        for k, v in zip(df["merge_Global_ID__c"], df["merge_Id"])
        if str(k).strip()
    }
    
    dict_email_name_to_merge = {
        (str(row["digital_Email"]).lower(), str(row["digital_Name"]).lower()): str(row["merge_Id"])
        for _, row in df.iterrows()
        if str(row["digital_Email"]).strip() and str(row["digital_Name"]).strip()
    }
    
    dict_digital_to_email_name = {
        str(row["digital_prod_Id"]).lower(): (
            str(row["digital_Email"]).lower(),
            str(row["digital_Name"]).lower(),
        )
        for _, row in df.iterrows()
        if str(row["digital_prod_Id"]).strip()
    }
    
    def map_user_id(digital_prod_id):
        """Map user ID using multi-step logic"""
        if not digital_prod_id or str(digital_prod_id).strip() == "":
            return ""
        
        digital_prod_id_lc = str(digital_prod_id).strip().lower()
        
        # Step 1: digital_prod_Id -> digital_Global_ID__c
        digital_global_id = dict_digital_to_global.get(digital_prod_id_lc, "").lower()
        
        # Step 2: digital_Global_ID__c -> merge_Id
        if digital_global_id and digital_global_id in dict_global_to_merge:
            return dict_global_to_merge[digital_global_id]
        
        # Fallback: Try email/name match
        email_name = dict_digital_to_email_name.get(digital_prod_id_lc)
        if email_name:
            email_name_lc = (email_name[0].lower(), email_name[1].lower())
            mapped = dict_email_name_to_merge.get(email_name_lc, "")
            if mapped:
                return mapped
        
        return ""
    
    return map_user_id


def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    if not os.path.exists(SOURCE_FILE):
        raise FileNotFoundError(f"Source file not found: {SOURCE_FILE}")
    
    print("ðŸ“– Loading lookup files...")
    map_user_id = load_user_lookup(USER_LOOKUP_FILE)
    dict_account = load_simple_lookup(ACCOUNT_LOOKUP_FILE)
    dict_contact = load_simple_lookup(CONTACT_LOOKUP_FILE)
    dict_case = load_simple_lookup(CASE_LOOKUP_FILE)
    
    # Prepare output files
    source_basename = os.path.splitext(os.path.basename(SOURCE_FILE))[0]
    main_output_file = os.path.join(OUTPUT_DIR, f"{source_basename}_mapped.csv")
    
    # Remove existing output files
    if os.path.exists(main_output_file):
        os.remove(main_output_file)
    
    # Track unmapped records per column
    unmapped_data = {
        "Previous_Owner__c": [],
        "Submitter__c": [],
        "CCd_Agent__c": [],
        "Requestor__c": [],
        "Original_Owner__c": [],
        "Customer__c": [],
        "Expedite_Supplier_Contact__c": [],
        "Customer_Contact__c": [],
        "ISR__c": [],
        "Case__c": [],
    }
    
    reader = pd.read_csv(SOURCE_FILE, dtype=str, chunksize=CHUNK_SIZE)
    header_written = False
    total_rows = 0
    
    print("ðŸ”„ Processing source file in chunks...")
    
    for chunk_idx, chunk in enumerate(reader, start=1):
        chunk = chunk.fillna("")
        
        # Store original values for unmapped tracking
        original_values = {}
        for col in unmapped_data.keys():
            if col == "Case__c":
                original_values[col] = chunk.get("Id", pd.Series([""] * len(chunk))).copy()
            elif col in chunk.columns:
                original_values[col] = chunk[col].copy()
        
        # === USER LOOKUP ===
        user_fields_group_a = ["OwnerId", "CreatedById", "LastModifiedById"]
        user_fields_group_b = ["Previous_Owner__c", "Submitter__c", "CCd_Agent__c", "Requestor__c", "Original_Owner__c"]
        
        # Group A: Always set defaults if blank/unmapped
        for col in user_fields_group_a:
            if col in chunk.columns:
                chunk[col] = chunk[col].apply(map_user_id)
                
                if col == "OwnerId":
                    mask = chunk[col].astype(str).str.strip() == ""
                    chunk.loc[mask, col] = DEFAULT_OWNER_ID
                else:  # CreatedById, LastModifiedById
                    mask = chunk[col].astype(str).str.strip() == ""
                    chunk.loc[mask, col] = DEFAULT_CREATEDBY_LASTMODIFIED_ID
        
        # Group B: Set default only if source had value but unmapped
        for col in user_fields_group_b:
            if col in chunk.columns:
                original = original_values[col].astype(str).str.strip()
                chunk[col] = chunk[col].apply(map_user_id)
                mapped = chunk[col].astype(str).str.strip()
                
                # If source had value but mapping failed, use default
                unmapped_mask = (original != "") & (mapped == "")
                chunk.loc[unmapped_mask, col] = DEFAULT_OWNER_ID
                
                # Track unmapped for reporting
                if unmapped_mask.any():
                    unmapped_rows = chunk[unmapped_mask].copy()
                    unmapped_rows[col] = original[unmapped_mask].values
                    unmapped_data[col].append(unmapped_rows)
        
        # === ACCOUNT LOOKUP ===
        if "Customer__c" in chunk.columns:
            original = original_values["Customer__c"].astype(str).str.strip()
            chunk["Customer__c"] = chunk["Customer__c"].apply(
                lambda val: dict_account.get(str(val).strip().lower(), "") if str(val).strip() else ""
            )
            mapped = chunk["Customer__c"].astype(str).str.strip()
            
            # Track unmapped
            unmapped_mask = (original != "") & (mapped == "")
            if unmapped_mask.any():
                unmapped_rows = chunk[unmapped_mask].copy()
                unmapped_rows["Customer__c"] = original[unmapped_mask].values
                unmapped_data["Customer__c"].append(unmapped_rows)
        
        # === CONTACT LOOKUP ===
        contact_fields = ["Expedite_Supplier_Contact__c", "Customer_Contact__c", "ISR__c"]
        for col in contact_fields:
            if col in chunk.columns:
                original = original_values[col].astype(str).str.strip()
                chunk[col] = chunk[col].apply(
                    lambda val: dict_contact.get(str(val).strip().lower(), "") if str(val).strip() else ""
                )
                mapped = chunk[col].astype(str).str.strip()
                
                # Track unmapped
                unmapped_mask = (original != "") & (mapped == "")
                if unmapped_mask.any():
                    unmapped_rows = chunk[unmapped_mask].copy()
                    unmapped_rows[col] = original[unmapped_mask].values
                    unmapped_data[col].append(unmapped_rows)
        
        # === CASE LOOKUP ===
        # Create new Case__c column from Id
        if "Id" in chunk.columns:
            original_id = original_values["Case__c"].astype(str).str.strip()
            chunk["Case__c"] = chunk["Id"].apply(
                lambda val: dict_case.get(str(val).strip().lower(), "") if str(val).strip() else ""
            )
            mapped_case = chunk["Case__c"].astype(str).str.strip()
            
            # Track unmapped
            unmapped_mask = (original_id != "") & (mapped_case == "")
            if unmapped_mask.any():
                unmapped_rows = chunk[unmapped_mask].copy()
                unmapped_rows["Case__c"] = original_id[unmapped_mask].values
                unmapped_data["Case__c"].append(unmapped_rows)
        else:
            chunk["Case__c"] = ""
        
        # === ADD BUSINESS HOURS ID ===
        chunk["BusinessHoursId"] = BUSINESS_HOURS_ID
        
        # === EXCLUDE COLUMNS ===
        for col in EXCLUDE_COLUMNS:
            if col in chunk.columns:
                chunk.drop(columns=[col], inplace=True)
        
        # Write to main output
        chunk.to_csv(
            main_output_file,
            index=False,
            mode="a" if header_written else "w",
            header=not header_written,
            encoding="utf-8-sig",
        )
        header_written = True
        total_rows += len(chunk)
        
        print(f"âœ… Chunk {chunk_idx}: {len(chunk)} rows processed")
    
    # === WRITE UNMAPPED FILES ===
    print("\nðŸ“ Writing unmapped reports...")
    unmapped_counts = {}
    
    for col, data_list in unmapped_data.items():
        if data_list:
            unmapped_df = pd.concat(data_list, ignore_index=True)
            unmapped_file = os.path.join(OUTPUT_DIR, f"{col}_unmapped.csv")
            unmapped_df.to_csv(unmapped_file, index=False, encoding="utf-8-sig")
            unmapped_counts[col] = len(unmapped_df)
            print(f"   âš ï¸ {col}: {len(unmapped_df)} unmapped â†’ {unmapped_file}")
        else:
            unmapped_counts[col] = 0
    
    # === SUMMARY ===
    print("\n" + "="*60)
    print("âœ… MAPPING COMPLETED!")
    print("="*60)
    print(f"ðŸ“Š Total rows processed: {total_rows}")
    print(f"ðŸ“ Main output: {main_output_file}")
    
    main_size_mb = os.path.getsize(main_output_file) / (1024 * 1024)
    print(f"ðŸ“ Output file size: {main_size_mb:.2f} MB")
    
    total_unmapped = sum(unmapped_counts.values())
    print(f"\nâš ï¸ Total unmapped records across all columns: {total_unmapped}")
    
    if total_unmapped > 0:
        print("\nUnmapped breakdown:")
        for col, count in unmapped_counts.items():
            if count > 0:
                print(f"   - {col}: {count}")


if __name__ == "__main__":
    main()

